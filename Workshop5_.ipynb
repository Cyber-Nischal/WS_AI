{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cyber-Nischal/WS_AI/blob/main/Workshop5_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6FblR83OrYE",
        "outputId": "29750d34-2016-4b1b-8d5d-072a3fc2e113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def cost_function(X,Y,W):\n",
        "  \"\"\" Parameters:\n",
        "  This function finds the Mean Square Error.\n",
        "  Input parameters:\n",
        "   X: Feature Matrix\n",
        "   Y: Target Matrix\n",
        "   W: Weight Matrix\n",
        "  Output Parameters:\n",
        "   cost: accumulated mean square error.\n",
        "  \"\"\"\n",
        "  y_pred = X.dot(W)\n",
        "  e = (y_pred - Y) ** 2\n",
        "  m = len(Y)\n",
        "  cost = 1/(2*m)*np.sum(e);\n",
        "\n",
        "  return cost\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "print(cost)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "  \"\"\"\n",
        "  Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "  Parameters:\n",
        "  X (numpy.ndarray): Feature matrix (m x n).\n",
        "  Y (numpy.ndarray): Target vector (m x 1).\n",
        "  W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "  alpha (float): Learning rate.\n",
        "  iterations (int): Number of iterations for gradient descent.\n",
        "  Returns:\n",
        "   tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values\n",
        "       .\n",
        "          W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "          cost_history (list): History of cost values over iterations.\n",
        "  \"\"\"\n",
        "  # Initialize cost history\n",
        "  cost_history = [0] * iterations\n",
        "  # Number of samples\n",
        "  m = len(Y)\n",
        "  for iteration in range(iterations):\n",
        "    # Step 1: Hypothesis Values\n",
        "    Y_pred = X.dot(W)\n",
        "    # Step 2: Difference between Hypothesis and Actual Y\n",
        "    loss = Y_pred - Y\n",
        "    # Step 3: Gradient Calculation\n",
        "    dw = (X.T.dot(loss))/m\n",
        "    # Step 4: Updating Values of W using Gradient\n",
        "    W_update = W - alpha * dw\n",
        "    # Step 5: New Cost Value\n",
        "    cost = cost_function(X,Y,W_update)\n",
        "    cost_history[iteration] = cost\n",
        "  return W_update, cost_history\n",
        "\n",
        "# Generate random test data\n",
        "np.random.seed(0) # For reproducibility\n",
        "X = np.random.rand(100, 3) # 100 samples, 3 features\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3) # Initial guess for parameters\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "wOVpf_trTrhI",
        "outputId": "722b6290-7923-4923-b38c-638f705cc905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cost_function' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3883638781.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Test the gradient_descent function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mfinal_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m# Print the final parameters and cost history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Parameters:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3883638781.py\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(X, Y, W, alpha, iterations)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mW_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Step 5: New Cost Value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mcost_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mW_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cost_function' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Worksheet II"
      ],
      "metadata": {
        "id": "71cJZUi__wp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    e = np.exp(z)\n",
        "    return e / np.sum(e, axis=1, keepdims=True)\n",
        "\n",
        "z = np.array([[1,2,3],[1,2,3]])\n",
        "print(softmax(z))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8nyqjTv_v1K",
        "outputId": "8e5b9693-2da9-465f-a701-d3816699e7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.09003057 0.24472847 0.66524096]\n",
            " [0.09003057 0.24472847 0.66524096]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def loss_softmax(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-10))\n",
        "\n",
        "y_true = np.array([0,0,1])\n",
        "y_pred = np.array([0.1,0.2,0.7])\n",
        "print(loss_softmax(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "movY_fAgAiWw",
        "outputId": "7ec9c197-6f5d-4d83-cab8-2f290f398618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3566749437958753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    e = np.exp(z)\n",
        "    return e / np.sum(e, axis=1, keepdims=True)\n",
        "\n",
        "def cost_softmax(X, y, W, b):\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = softmax(z)\n",
        "    return -np.sum(y * np.log(y_pred + 1e-10)) / X.shape[0]\n",
        "\n",
        "X = np.array([[1,2],[2,3],[3,4]])\n",
        "y = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
        "W = np.zeros((2,3))\n",
        "b = np.zeros(3)\n",
        "\n",
        "print(cost_softmax(X, y, W, b))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26TiXcKYAkfA",
        "outputId": "026387e3-e414-45e5-8501-f16deadb8c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0986122883681098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    e = np.exp(z)\n",
        "    return e / np.sum(e, axis=1, keepdims=True)\n",
        "\n",
        "def compute_gradient_softmax(X, y, W, b):\n",
        "    n = X.shape[0]\n",
        "    y_pred = softmax(np.dot(X, W) + b)\n",
        "    grad_W = np.dot(X.T, (y_pred - y)) / n\n",
        "    grad_b = np.sum(y_pred - y, axis=0) / n\n",
        "    return grad_W, grad_b\n",
        "\n",
        "X = np.array([[1,2],[2,3],[3,4]])\n",
        "y = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
        "W = np.zeros((2,3))\n",
        "b = np.zeros(3)\n",
        "\n",
        "print(compute_gradient_softmax(X, y, W, b))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHXzyOl4Anso",
        "outputId": "f1516cbc-e1e3-4c56-a1eb-9418905147ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([[ 3.33333333e-01, -9.25185854e-17, -3.33333333e-01],\n",
            "       [ 3.33333333e-01, -7.40148683e-17, -3.33333333e-01]]), array([-3.70074342e-17, -3.70074342e-17, -3.70074342e-17]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    e = np.exp(z)\n",
        "    return e / np.sum(e, axis=1, keepdims=True)\n",
        "\n",
        "def cost_softmax(X, y, W, b):\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = softmax(z)\n",
        "    return -np.sum(y * np.log(y_pred + 1e-10)) / X.shape[0]\n",
        "\n",
        "def compute_gradient_softmax(X, y, W, b):\n",
        "    n = X.shape[0]\n",
        "    y_pred = softmax(np.dot(X, W) + b)\n",
        "    return np.dot(X.T, (y_pred - y)) / n, np.sum(y_pred - y, axis=0) / n\n",
        "\n",
        "def gradient_descent_softmax(X, y, W, b, alpha, n_iter):\n",
        "    for _ in range(n_iter):\n",
        "        gW, gb = compute_gradient_softmax(X, y, W, b)\n",
        "        W -= alpha * gW\n",
        "        b -= alpha * gb\n",
        "    return W, b\n",
        "\n",
        "X = np.array([[1,2],[2,3],[3,4]])\n",
        "y = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
        "W = np.zeros((2,3))\n",
        "b = np.zeros(3)\n",
        "\n",
        "W, b = gradient_descent_softmax(X, y, W, b, 0.1, 500)\n",
        "print(W, b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlyK0YAOAqI9",
        "outputId": "27e6fac6-3b67-44e9-b3cb-217b9c0aa2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-2.40349375  0.13118344  2.27231031]\n",
            " [ 0.45363773  0.09304183 -0.54667956]] [ 2.85713148 -0.03814162 -2.81898986]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    e = np.exp(z)\n",
        "    return e / np.sum(e, axis=1, keepdims=True)\n",
        "\n",
        "def predict_softmax(X, W, b):\n",
        "    return np.argmax(softmax(np.dot(X, W) + b), axis=1)\n",
        "\n",
        "X = np.array([[1,2],[2,3],[3,4]])\n",
        "W = np.array([[1,0,-1],[0,1,-1]])\n",
        "b = np.zeros(3)\n",
        "\n",
        "print(predict_softmax(X, W, b))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b66-8PZvAsnq",
        "outputId": "351c55d2-1be5-485a-a856-77281ee12070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_classification(y_true, y_pred):\n",
        "    c = len(np.unique(y_true))\n",
        "    cm = np.zeros((c,c), dtype=int)\n",
        "    for t,p in zip(y_true,y_pred):\n",
        "        cm[t,p] += 1\n",
        "    precision = np.mean([cm[i,i]/np.sum(cm[:,i]) if np.sum(cm[:,i])>0 else 0 for i in range(c)])\n",
        "    recall = np.mean([cm[i,i]/np.sum(cm[i,:]) if np.sum(cm[i,:])>0 else 0 for i in range(c)])\n",
        "    f1 = 2*precision*recall/(precision+recall) if precision+recall>0 else 0\n",
        "    return cm, precision, recall, f1\n",
        "\n",
        "y_true = np.array([0,1,2])\n",
        "y_pred = np.array([0,2,2])\n",
        "\n",
        "print(evaluate_classification(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yAfLgBgAu91",
        "outputId": "a5b12c44-0c97-42d7-e049-e8dad0b8d4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([[1, 0, 0],\n",
            "       [0, 0, 1],\n",
            "       [0, 0, 1]]), np.float64(0.5), np.float64(0.6666666666666666), np.float64(0.5714285714285715))\n"
          ]
        }
      ]
    }
  ]
}